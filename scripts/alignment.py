import json


def similarity(text1, text2):
    """
    T√≠nh ƒë·ªô gi·ªëng nhau gi·ªØa 2 chu·ªói (0-1)
    """
    from difflib import SequenceMatcher
    return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

def load_cache(cache_path):
    """
    ƒê·ªçc file cache
    
    Returns:
        dict: {filename: [{transcription, points, difficult}]}
    """
    cache_data = {}
    with open(cache_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            
            parts = line.split('\t', 1)
            if len(parts) == 2:
                filename = parts[0]
                ocr_data = json.loads(parts[1])
                cache_data[filename] = ocr_data
    
    return cache_data


def load_ground_truth_txt(txt_path):
    """
    ƒê·ªçc file TXT ground truth
    
    Returns:
        list: Danh s√°ch c√°c d√≤ng text
    """
    with open(txt_path, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]
    return lines


def align_gt_with_cache(ground_truth_lines, cache_boxes, similarity_threshold=0.3):
    """
    D√≥ng h√†ng ground truth v·ªõi cache bbox b·∫±ng thu·∫≠t to√°n similarity matching
    
    Args:
        ground_truth_lines: List text ground truth
        cache_boxes: List bbox t·ª´ cache
        similarity_threshold: Ng∆∞·ª°ng t∆∞∆°ng ƒë·ªìng t·ªëi thi·ªÉu (0-1)
    
    Returns:
        list: Cache m·ªõi v·ªõi text ground truth ƒë√£ ƒë∆∞·ª£c d√≥ng h√†ng
    """
    new_cache = []
    used_gt_indices = set()
    
    num_gt = len(ground_truth_lines)
    num_boxes = len(cache_boxes)
    
    print(f"üîç B·∫ÆT ƒê·∫¶U ALIGNMENT:")
    print(f"   - Ground truth: {num_gt} d√≤ng")
    print(f"   - Cache bbox:   {num_boxes} boxes")
    print(f"   - Ng∆∞·ª°ng similarity: {similarity_threshold}\n")
    
    # Duy·ªát qua t·ª´ng bbox trong cache
    for box_idx, cache_box in enumerate(cache_boxes):
        ocr_text = cache_box['transcription']
        best_match_idx = -1
        best_similarity = 0
        
        # T√¨m ground truth gi·ªëng nh·∫•t v·ªõi OCR text
        for gt_idx, gt_text in enumerate(ground_truth_lines):
            if gt_idx in used_gt_indices:
                continue
            
            sim = similarity(ocr_text, gt_text)
            
            if sim > best_similarity:
                best_similarity = sim
                best_match_idx = gt_idx
        
        # Quy·∫øt ƒë·ªãnh match hay gi·ªØ nguy√™n
        if best_match_idx != -1 and best_similarity >= similarity_threshold:
            matched_text = ground_truth_lines[best_match_idx]
            used_gt_indices.add(best_match_idx)
            
            new_cache.append({
                'transcription': matched_text,
                'points': cache_box['points'],
                'difficult': cache_box.get('difficult', False)
            })
            
            if best_similarity < 0.9:  # Ch·ªâ hi·ªÉn th·ªã n·∫øu kh√¥ng match ho√†n to√†n
                print(f"   [{box_idx+1}] Similarity={best_similarity:.2f}")
                print(f"       OCR: {ocr_text[:50]}...")
                print(f"       GT:  {matched_text[:50]}...\n")
        else:
            # Kh√¥ng t√¨m th·∫•y match ƒë·ªß t·ªët, gi·ªØ nguy√™n OCR text
            new_cache.append(cache_box)
            print(f"   [{box_idx+1}] ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y match (best={best_similarity:.2f})")
            print(f"       Gi·ªØ nguy√™n OCR: {ocr_text[:50]}...\n")
    
    # C·∫£nh b√°o n·∫øu c√≥ GT ch∆∞a ƒë∆∞·ª£c d√πng
    unused_gt = num_gt - len(used_gt_indices)
    if unused_gt > 0:
        print(f"\n‚ö†Ô∏è  C√≥ {unused_gt} d√≤ng ground truth CH∆ØA ƒë∆∞·ª£c match!")
    
    print(f"‚úÖ ƒê√£ align {len(new_cache)} boxes")
    
    return new_cache


def save_new_cache(cache_data, output_path):
    """
    L∆∞u cache m·ªõi
    Format: filename\t[{transcription, points, difficult}]
    """
    import os
    
    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        print(f"üìÅ ƒê√£ t·∫°o th∆∞ m·ª•c: {output_dir}")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for filename, boxes in cache_data.items():
            json_str = json.dumps(boxes, ensure_ascii=False)
            f.write(f"{filename}\t{json_str}\n")
    
    print(f"‚úÖ ƒê√£ l∆∞u cache m·ªõi: {output_path}")


def compare_old_new(old_boxes, new_boxes, num_show=5):
    """
    So s√°nh cache c≈© vs m·ªõi
    """
    print("=" * 80)
    print("SO S√ÅNH TEXT C≈® (OCR) vs M·ªöI (GROUND TRUTH)")
    print("=" * 80)
    
    for i in range(min(num_show, len(old_boxes), len(new_boxes))):
        old_text = old_boxes[i]['transcription']
        new_text = new_boxes[i]['transcription']
        
        if old_text != new_text:
            print(f"\n[D√≤ng {i+1}]")
            print(f"  C≈©: {old_text}")
            print(f"  M·ªõi: {new_text}")
            print(f"  BBox gi·ªØ nguy√™n: {new_boxes[i]['points'][0][:2]}...")


def process_single_page(cache_path, gt_txt_path, page_name, output_path):
    """
    X·ª≠ l√Ω 1 trang: thay text trong cache b·∫±ng ground truth
    
    Args:
        cache_path: ƒê∆∞·ªùng d·∫´n file cache g·ªëc
        gt_txt_path: ƒê∆∞·ªùng d·∫´n file TXT ground truth
        page_name: T√™n trang trong cache (vd: 'test-png/page_130.png')
        output_path: ƒê∆∞·ªùng d·∫´n file cache m·ªõi
    """
    print(f"üìÅ ƒêang x·ª≠ l√Ω trang: {page_name}\n")
    
    # 1. Load cache
    print("1Ô∏è‚É£  Load cache g·ªëc...")
    cache_data = load_cache(cache_path)
    
    if page_name not in cache_data:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y {page_name} trong cache!")
        print(f"   C√°c trang c√≥ s·∫µn:")
        for name in cache_data.keys():
            print(f"   - {name}")
        return
    
    old_boxes = cache_data[page_name]
    print(f"   ‚úÖ T√¨m th·∫•y {len(old_boxes)} boxes\n")
    
    # 2. Load ground truth
    print("2Ô∏è‚É£  Load ground truth TXT...")
    gt_lines = load_ground_truth_txt(gt_txt_path)
    print(f"   ‚úÖ ƒê·ªçc ƒë∆∞·ª£c {len(gt_lines)} d√≤ng\n")
    
    # 3. D√≥ng h√†ng
    print("3Ô∏è‚É£  D√≥ng h√†ng ground truth v·ªõi bbox...")
    new_boxes = align_gt_with_cache(gt_lines, old_boxes)
    print(f"   ‚úÖ ƒê√£ map {len(new_boxes)} d√≤ng\n")
    
    # 4. So s√°nh
    compare_old_new(old_boxes, new_boxes, num_show=5)
    
    # 5. L∆∞u cache m·ªõi
    print(f"\n4Ô∏è‚É£  L∆∞u cache m·ªõi...")
    new_cache_data = cache_data.copy()
    new_cache_data[page_name] = new_boxes
    save_new_cache(new_cache_data, output_path)
    
    print(f"\nüéâ Ho√†n th√†nh! ƒê√£ thay {len(new_boxes)} text boxes")


def process_multiple_pages(cache_path, gt_folder, output_path):
    """
    X·ª≠ l√Ω nhi·ªÅu trang c√πng l√∫c
    
    Args:
        cache_path: File cache g·ªëc
        gt_folder: Th∆∞ m·ª•c ch·ª©a c√°c file TXT ground truth
        output_path: File cache m·ªõi
        
    Quy ∆∞·ªõc ƒë·∫∑t t√™n file TXT:
        Cache: 'test-png/page_130.png'
        TXT:   'gt_folder/page_130.txt'
    """
    import os
    
    cache_data = load_cache(cache_path)
    new_cache_data = cache_data.copy()
    
    for page_name in cache_data.keys():
        # T·∫°o t√™n file TXT t∆∞∆°ng ·ª©ng
        base_name = os.path.splitext(os.path.basename(page_name))[0]
        txt_file = os.path.join(gt_folder, f"{base_name}.txt")
        
        if not os.path.exists(txt_file):
            print(f"‚ö†Ô∏è  B·ªè qua {page_name}: kh√¥ng t√¨m th·∫•y {txt_file}")
            continue
        
        print(f"\n{'='*80}")
        print(f"X·ª≠ l√Ω: {page_name}")
        print(f"{'='*80}")
        
        gt_lines = load_ground_truth_txt(txt_file)
        old_boxes = cache_data[page_name]
        new_boxes = align_gt_with_cache(gt_lines, old_boxes)
        new_cache_data[page_name] = new_boxes
        
        print(f"‚úÖ ƒê√£ map {len(new_boxes)} d√≤ng cho {page_name}")
    
    save_new_cache(new_cache_data, output_path)
    print(f"\nüéâ Ho√†n th√†nh t·∫•t c·∫£!")


if __name__ == "__main__":
    # ===== C√ÅCH 1: X·ª≠ l√Ω 1 trang =====
    # ƒê·ªçc cache c≈©, s·ª≠a text, ghi ƒë√® l·∫°i v√†o ch√≠nh file cache ƒë√≥
    cache_path = '/Users/huonglam/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master/T·ªët nghi·ªáp/VOCR/data/Cache.cach'
    cache_new = 'data/algin/cache.cach'
    process_single_page(
        cache_path=cache_path,
        gt_txt_path='/Users/huonglam/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master/T·ªët nghi·ªáp/VOCR/data/raw/daivietsuky-text.txt',
        page_name='test-png/daivietsuky-image_page_130.png',
        output_path=  cache_new
    )